# config/benchmark/livedrbench.yaml
# Microsoft LiveDRBench — deep research claim discovery benchmark
# 100 tasks across 8 categories, encrypted answers
# Evaluation: run agent → export predictions → use LiveDRBench evaluate.py
defaults:
  - default
  - _self_

name: "livedrbench"

data:
  data_dir: "${data_dir}/livedrbench"

execution:
  max_tasks: -1        # run all tasks
  max_concurrent: 2    # conservative — heavy research tasks with many tool calls
  pass_at_k: 1

openai_api_key: "${oc.env:OPENAI_API_KEY,???}"
